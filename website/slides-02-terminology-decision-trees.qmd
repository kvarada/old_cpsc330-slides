---
title: 'Lecture 2: Terminology, Baselines, Decision Trees'
description: Terminology, decision Trees
description-short: 'Supervised machine learning terminology: Features, target, examples, training, parameters and hyperparameters, Decision boundary, classification vs. regression, inference vs. prediction, accuracy vs. error, baselines, intuition of decision trees'
format:
  revealjs:
    slide-number: true
    smaller: true
    center: true
    logo: img/UBC-CS-logo.png
    resources:
      - data/
      - img/
---

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import sys
sys.path.append(os.path.join(os.path.abspath("."), "code"))
from IPython.display import HTML, display
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline

plt.rcParams["font.size"] = 16
pd.set_option("display.max_colwidth", 200)
%matplotlib inline

DATA_DIR = 'data/' 
```

## Announcements 

- Things due this week 
    - Homework 1 (hw1): Due Sept 10 11:59pm 
- Homework 2 (hw2) has been released (Due: Sept 18, 11:59pm)
    - There is some autograding in this homework. 
- You can find the tentative due dates for all deliverables [here](https://ubc-cs.github.io/cpsc330-2024W1/README.html#deliverable-due-dates-tentative).
- Please monitor Piazza (especially pinned posts and instructor posts) for announcements. 
- I'll assume that you've watched the pre-lecture videos. 

## Recap: What is ML? 

- ML uses data to build models that find patterns, make predictions, or generate content.
- It helps computers learn from data to make decisions.
- No one model works for every situation.

## Recap: Supervised learning

- We wish to find a model function $f$ that relates $X$ to $y$.
- We use the model function to predict targets of new examples. 

![](img/sup-learning.png){.nostretch fig-align="center" width="700px"}

In the first part of this course, we'll focus on supervised machine learning. 

## Framework

- There are many frameworks to do do machine learning. 
- We'll mainly be using [`scikit-learn` framework](https://scikit-learn.org/stable/). 
```{python}
import IPython
url = "https://scikit-learn.org"
IPython.display.IFrame(width=1000, height=650, src=url)
```

::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
## Running example 

Imagine that you're in this lucky situation where after graduating, you've a number of a few job offers and you want to make a decision on which one to pick.  

```{python}
#| echo: true
toy_happiness_df = pd.read_csv(DATA_DIR + 'toy_job_happiness.csv')
toy_happiness_df
```
:::

# Terminology

## Features, target, example
- What are the **features** $X$? 
  - features = inputs = predictors = explanatory variables = regressors = independent variables = covariates 
- What's the target $y$?
  - target = output = outcome = response variable = dependent variable = labels 
- Can you think of other relevant features for this problem? 
- What is an example?

## Classification vs. Regression
- Is this a **classification** problem or a **regression** problem?  

```{python}
toy_happiness_df
```

## Prediction vs. Inference
- **Inference** is using the model to understand the relationship between the features and the target 
  - Why certain factors influence happiness? 
- **Prediction** is using the model to predict the target value for new examples based on learned patterns.
- Of course these goals are related, and in many situations we need both. 

## Training 
- In supervised ML, the goal is to learn a function that maps input features ($X$) to a target ($y$).
- The relationship between $X$ and $y$ is often complex, making it difficult to  define mathematically.
- We use algorithms to approximate this complex relationship between $X$ and $y$.
- **Training** is the process of applying an algorithm to learn the best function (or model) that maps $X$ to $y$. 
- In this course, I'll help you build an intuition for how these models work. 

::: {.scroll-container style="overflow-y: scroll; height: 400px;"}
## Baseline
- What's the simplest possible algorithm? 
  - Predict the most popular target.  

```{python}
#| echo: true
from sklearn.dummy import DummyClassifier

X = toy_happiness_df.drop(columns=["happy?"])
y = toy_happiness_df["happy?"]

model = DummyClassifier() # Define a class object of the model 
model.fit(X, y)
toy_happiness_df['dummy_predictions'] = model.predict(X) 
toy_happiness_df
```
:::

# Decision trees 
## Decision tree with `sklearn`
Let's train a simple decision tree on our toy dataset.  

```{python}
#| echo: true
from sklearn.tree import DecisionTreeClassifier # import the classifier
from sklearn.tree import plot_tree

model = DecisionTreeClassifier(max_depth=2) # Create a class object
model.fit(X, y)
plot_tree(model, filled=True, feature_names = X.columns, impurity = False, fontsize=16);

```



## Intuition

## Prediction 

## Training (high level)

## Parameters vs. Hyperparameters 

## Decision boundary 


## iClicker 

## HW2 Worksheet portion 

